# ðŸ§  Assistant IA AcadÃ©mique â€” RAG + Ollama + Streamlit

Assistant intelligent acadÃ©mique capable de :

- rÃ©pondre Ã  des questions Ã  partir de **documents internes** (RAG),
- exÃ©cuter des **calculs** (calculatrice sÃ©curisÃ©e),
- donner la **mÃ©tÃ©o**,
- faire des **recherches web** (avec consentement explicite),
- gÃ©rer une **TODO-list persistante**,
- envoyer la **derniÃ¨re rÃ©ponse par e-mail**,

le tout orchestrÃ© avec **Ollama**, **LangChain** et **Streamlit**.

---

## âœ… Objectifs du projet (cÃ´tÃ© TP)

Ce projet rÃ©pond aux exigences :

- **RAG complet** sur des fichiers locaux (cours acadÃ©miques).
- **Agents / outils** : calculatrice, mÃ©tÃ©o, recherche web, TODO.
- **Routage intelligent** : choix automatique entre RAG, outils, smalltalk.
- **MÃ©moire conversationnelle** persistante.
- **Interface conversationnelle** avec Streamlit.
- **Recherche web** intÃ©grÃ©e (avec consentement utilisateur).
- **Envoi dâ€™e-mails** de la derniÃ¨re rÃ©ponse.
- Code structurÃ©, versionnÃ©, avec documentation dâ€™architecture.

---

## ðŸš€ 1. FonctionnalitÃ©s principales

### ðŸ”¹ 1.1. RAG (Retrieval-Augmented Generation)

- Charge automatiquement les documents du dossier `RAG_Data/`
- Supporte les formats : `.txt`, `.pdf`, `.docx`
- Indexation dans un **vector store Chroma** persistant : `chroma_store/`
- Embeddings : `sentence-transformers/all-MiniLM-L6-v2`
- LLM local : **Ollama** (`llama3.2:3b`)
- **ContrÃ´le des hallucinations** :
  - test de recouvrement lexical (_has_lexical_overlap),
  - si le contexte ne parle pas clairement de la question â†’ rÃ©ponse EXACTE :  
    `Je ne sais pas.`

ðŸ§  **Logique de prioritÃ© :**

1. La question part **dâ€™abord** dans le RAG (documents internes).
2. Si rien de pertinent nâ€™est trouvÃ© :
   - lâ€™assistant rÃ©pond :  
     `Je nâ€™ai rien trouvÃ© dans les documents internes. Veux-tu que je cherche sur le web ? RÃ©ponds par oui ou non.`
   - si lâ€™utilisateur rÃ©pond **oui** â†’ recherche web,
   - si **non** â†’ lâ€™assistant reste sur les docs internes / smalltalk.

---

### ðŸ”¹ 1.2. Outils intÃ©grÃ©s (Agents)

Les outils sont implÃ©mentÃ©s dans `agents.py`, et sÃ©lectionnÃ©s automatiquement via le routeur `router.py`.

#### ðŸ§® Calculatrice intelligente

- Comprend des expressions comme :
  - `2 + 3 * 4`
  - `2^8`
  - `23Â²`, `10Â³`
  - `sqrt16`, `log10(100)`, `exp2`
  - `sin45`, `cos30`, `tan60`, `sin 45Â°`, `cos 30deg`
- Normalisations automatiques :
  - `,` â†’ `.`  
  - `^` â†’ `**`  
  - `Ã—`, `Ã·`, `âˆ’`, `â€“` â†’ `*`, `/`, `-`
  - conversion degrÃ©s â†’ radians (`sin30Â°` â†’ `sin(0.5235...)`)
- SÃ©curisÃ©e :
  - pas de `eval` Python,
  - parsing via **AST**,
  - seules certaines opÃ©rations / fonctions / constantes sont autorisÃ©es.

#### ðŸŒ¦ï¸ MÃ©tÃ©o

- Comprend des requÃªtes en langage naturel :
  - `quel temps fait-il Ã  Lyon ?`
  - `donne-moi la mÃ©tÃ©o pour Nice aujourd'hui`
  - `meteo paris`
- Ã‰tapes :
  1. Normalisation du nom de ville (`_normalize_city_free_text`).
  2. GÃ©ocodage via **Nominatim (OpenStreetMap)**.
  3. MÃ©tÃ©o actuelle via **Open-Meteo**.
  4. Fallback sur un petit dictionnaire interne (`Paris`, `Lyon`, `Marseille`, etc.) si les APIs externes Ã©chouent.

#### ðŸ” Recherche web

- Utilise **DuckDuckGo Search** via la librairie `ddgs`.
- Deux maniÃ¨res de lâ€™utiliser :
  - **explÃ­cite** :  
    `recherche sur le web la cuisine italienne`  
    `cherche sur internet les rÃ©seaux de neurones`
  - **aprÃ¨s Ã©chec du RAG** (avec consentement) :
    - lâ€™assistant demande **oui/non**
    - si **oui**, il affiche une liste de rÃ©sultats formatÃ©s (titre + lien + extrait).

#### ðŸ“ Gestion TODO

- Commandes en langage naturel :
  - `ajoute : rÃ©viser IA`
  - `ajoute rÃ©viser rÃ©seaux`
  - `liste` / `list`
  - `termine 2` / `done: 2`
- Les tÃ¢ches sont stockÃ©es dans `todo_store.json` (persistance entre les sessions).
- Lâ€™interface Streamlit reformate le JSON en liste lisible avec :
  - âœ… tÃ¢ches terminÃ©es  
  - ðŸ”¹ tÃ¢ches en cours

#### ðŸ’¬ Smalltalk

- GÃ¨re les salutations simples :
  - `bonjour`, `salut`, `coucou`, `bonsoir`, `hello`, `hey`â€¦
- Utilise un LLM local via Ollama (`llama3.2:3b`) avec un prompt simple :
  > "Tu es un assistant amical et bref."

---

### ðŸ”¹ 1.3. Envoi de la derniÃ¨re rÃ©ponse par e-mail

- Configuration dans `.env` :

```env
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=ton.email@gmail.com
SMTP_PASS=mot_de_passe_application
SMTP_FROM=ton.email@gmail.com


### ðŸ”¹ 3. Envoi de la derniÃ¨re rÃ©ponse par e-mail

- ConfigurÃ© via `.env` :

```env
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=ton.email@gmail.com
SMTP_PASS=mot_de_passe_application
SMTP_FROM=ton.email@gmail.com

projet-ia-assistant-rag-academique/
â”‚
â”œâ”€â”€ app.py                 # Application Streamlit (UI + orchestration RAG / tools / web / e-mail)
â”œâ”€â”€ agents.py              # Outils : calculatrice, mÃ©tÃ©o, recherche web, TODO
â”œâ”€â”€ router.py              # DÃ©tection dâ€™intention (calc / mÃ©tÃ©o / web / rag / todo / smalltalk)
â”œâ”€â”€ rag_core.py            # Moteur RAG (Chroma + embeddings + Ollama)
â”œâ”€â”€ rag.py                 # (optionnel) API simplifiÃ©e autour du moteur RAG
â”œâ”€â”€ reindex_once.py        # Script pour forcer une rÃ©indexation des documents
â”‚
â”œâ”€â”€ RAG_Data/              # Documents internes utilisÃ©s par le RAG
â”‚   â”œâ”€â”€ Cours_IA.txt
â”‚   â”œâ”€â”€ Cours_Pytho.txt
â”‚   â””â”€â”€ Cours_Reseaux.txt
â”‚
â”œâ”€â”€ chroma_store/          # Index vectoriel persistant (crÃ©Ã© automatiquement)
â”œâ”€â”€ todo_store.json        # Stockage persistant des tÃ¢ches TODO
â”œâ”€â”€ memory_store.json      # Historique de conversation (chat) persistant
â”‚
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ .env                   # Variables dâ€™environnement (SMTP, etc.)
â”œâ”€â”€ .gitignore             # Exclusions Git
â””â”€â”€ README.md              # Documentation du projet

pip install -r requirements.txt
ollama run llama3.2:3b
streamlit run app.py

